/usr/local/zookeeper/bin/zkServer.sh start
/usr/local/hadoop/sbin/start-all.sh
mapred --daemon start historyserver
/usr/local/zookeeper/bin/zkCli.sh -server node1:2181

hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.4-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 10MB
hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.4-tests.jar TestDFSIO -clean
hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.4.jar pi 2 10

hdfs oiv -i /usr/local/hadoop/tmp/dfs/name/current/fsimage_0000000000000585502 -p XML -o my_fsimage.xml
hdfs oev -i /usr/local/hadoop/tmp/dfs/name/current/edits_0000000000000000001-0000000000000000153 -p XML -o myedits.xml

hive -e " select * from mytest.dept" | sed 's/[\t]/,/g'  > /root/data/hhd.csv
 

hadoop.dll 下载
hadoop-ha
sparkalone-ha 搭建
spark pom 依赖
kafka eagle
tez
hive+hbase
sqoop+hbase
structured streaming的groupby
mllib als 设置列名
azkaban
hive 导出到 mysql
/usr/local/kafka/bin/kafka-topics.sh --create --zookeeper node1:2181,node2:2181,node3:2181 --topic _HOATLASOK --partitions 3 --replication-factor 3
/usr/local/kafka/bin/kafka-topics.sh --create --zookeeper node1:2181,node2:2181,node3:2181 --topic ATLAS_ENTITIES --partitions 3 --replication-factor 3
/usr/local/atlas/bin/atlas_start.py


https://pan.baidu.com/s/18Feqa_63640xPB0fYJ8Ttg
9bnr
https://mp.weixin.qq.com/s/0mgy07WAMBYNBP6er8_hDA mst



/usr/local/spark/sbin/start-all.sh
/usr/local/spark/sbin/start-history-server.sh
/usr/local/spark/bin/spark-shell --master spark://node1:7077
/usr/local/spark/sbin/stop-all.sh
/usr/local/spark/sbin/stop-history-server.sh

/usr/local/spark/bin/spark-submit --master yarn --deploy-mode client --driver-memory 512m --driver-cores 1 --executor-memory 512m --num-executors 2 --executor-cores 1 --class org.apache.spark.examples.SparkPi /usr/local/spark/examples/jars/spark-examples_2.12-3.0.2.jar 10
/usr/local/spark/bin/spark-submit --master yarn --deploy-mode cluster --driver-memory 512m --driver-cores 1 --executor-memory 512m --num-executors 2 --executor-cores 1 --class spark_wordcount.WordCount /root/jar_test/spark_api-1.0-SNAPSHOT.jar hdfs://node1:8020/input/wordcount/wordcount.txt hdfs://node1:8020/output/wordcount_out3
/usr/local/spark/bin/spark-submit --master yarn --deploy-mode client --driver-memory 512m --driver-cores 1 --executor-memory 512m --num-executors 2 --executor-cores 1 --class structured_operation.Demo04_Operation /root/jar_test/structured_streaming-1.0-SNAPSHOT.jar

nohup /usr/local/hive/bin/hive --service metastore &
/usr/local/spark/bin/spark-sql

/usr/local/spark/sbin/start-thriftserver.sh --hiveconf hive.server2.thrift.port=10000 --hiveconf hive.server2.thrift.bind.host=node3 --master local[*]
/usr/local/spark/bin/beeline 
!connect jdbc:hive2://node3:10000
/usr/local/spark/sbin/stop-thriftserver.sh

redis-server& /usr/local/redis/redis.conf
redis-cli



/usr/local/azkaban/azkaban-exec-server/bin/start-exec.sh
/usr/local/azkaban/azkaban-exec-server/bin/shutdown-exec.sh
curl -G "node1:12321/executor?action=activate" && echo
curl -G "node2:12321/executor?action=activate" && echo
curl -G "node3:12321/executor?action=activate" && echo
/usr/local/azkaban/azkaban-web-server/bin/start-web.sh
/usr/local/azkaban/azkaban-web-server/bin/shutdown-web.sh
ozjzvddayzdgbeej



/usr/local/flink/bin/start-cluster.sh
/usr/local/flink/bin/stop-cluster.sh
/usr/local/flink/bin/historyserver.sh start
/usr/local/flink/bin/historyserver.sh stop
/usr/local/flink/bin/flink run /usr/local/flink/examples/batch/WordCount.jar --input /root/data/wordcount.txt --output /root/data/wordcount_out2
/usr/local/flink/bin/yarn-session.sh -n 2 -tm 800 -s 1 -d
yarn application -kill application_1618719179222_0006
/usr/local/flink/bin/flink run -m yarn-cluster -yjm 1024 -ytm 1024 /usr/local/flink/examples/batch/WordCount.jar



/usr/local/flume/bin/flume-ng agent --conf /usr/local/flume/conf --conf-file /usr/local/flume/job/netcat-flume-logger.conf --name a1 -Dflume.root.logger=INFO,console
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/netcat-flume-logger.conf -n a1 -Dflume.root.logger=INFO,console
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/file-flume-logger.conf -n a1 -Dflume.root.logger=INFO,console
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/file-flume-hdfs.conf -n a1
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/dir-flume-hdfs.conf -n a1
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/netcat-flume-hdfs.conf -n a1 -Dflume.root.logger=INFO,console
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/files-flume-logger.conf -n a1 -Dflume.root.logger=INFO,console
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/mysource.conf -n a1 -Dflume.root.logger=INFO,console
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/mysink.conf -n a1 -Dflume.root.logger=INFO,console
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/netcat-flume-logger.conf -n a1 -Dflume.root.logger=INFO,console -Dflume.monitoring.type=ganglia -Dflume.monitoring.hosts=node1:8649
group1（a2,node1）（a3,node2）（a1,node3）
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/group1/flume1.conf -n a1
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/group1/flume2.conf -n a2
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/group1/flume3.conf -n a3
group2（a2,node1）（a3,node2）（a1,node3）
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/group2/flume1.conf -n a1
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/group2/flume2.conf -n a2 -Dflume.root.logger=INFO,console
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/group2/flume3.conf -n a3 -Dflume.root.logger=INFO,console
group3（a2,node1）（a3,node2）（a1,node3）
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/group3/flume1.conf -n a1
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/group3/flume2.conf -n a2 -Dflume.root.logger=INFO,console
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/group3/flume3.conf -n a3 -Dflume.root.logger=INFO,console
group4（a2,node1）（a3,node2）（a1,node3）
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/group4/flume1.conf -n a1
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/group4/flume2.conf -n a2
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/group4/flume3.conf -n a3 -Dflume.root.logger=INFO,console
group5（a2,node1）（a3,node2）（a1,node3）
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/group5/flume1.conf -n a1
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/group5/flume2.conf -n a2 -Dflume.root.logger=INFO,console
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/group5/flume3.conf -n a3 -Dflume.root.logger=INFO,console
flume + kafka
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/flume-kafka.conf -n a1
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/type-kafka.conf -n a1




/usr/local/kafka/bin/kafka-server-start.sh -daemon /usr/local/kafka/config/server.properties
/usr/local/kafka/bin/kafka-server-stop.sh /usr/local/kafka/config/server.properties
/usr/local/kafka/bin/kafka-topics.sh --list --zookeeper node1:2181
/usr/local/kafka/bin/kafka-topics.sh --create --zookeeper node1:2181 --topic first --partitions 2 --replication-factor 2
/usr/local/kafka/bin/kafka-topics.sh --delete --zookeeper node1:2181 --topic first
/usr/local/kafka/bin/kafka-topics.sh --describe --zookeeper node1:2181 --topic second
/usr/local/kafka/bin/kafka-console-producer.sh --topic first --broker-list node1:9092
/usr/local/kafka/bin/kafka-console-consumer.sh --topic first --bootstrap-server node1:9092 --consumer.config /usr/local/kafka/config/consumer.properties
/usr/local/kafka/bin/kafka-console-consumer.sh --topic first --bootstrap-server node1:9092 --from-beginning
/usr/local/kafka/bin/kafka-console-consumer.sh --topic __consumer_offsets --bootstrap-server node1:9092 --formatter "kafka.coordinator.group.GroupMetadataManager\$OffsetsMessageFormatter" --consumer.config /usr/local/kafka/config/consumer.properties --from-beginning



/usr/local/hbase/bin/hbase-daemon.sh start master
/usr/local/hbase/bin/hbase-daemon.sh start regionserver
/usr/local/hbase/bin/hbase-daemon.sh stop master
/usr/local/hbase/bin/hbase-daemon.sh stop regionserver
/usr/local/hbase/bin/start-hbase.sh
/usr/local/hbase/bin/stop-hbase.sh
/usr/local/hbase/bin/hbase shell
/usr/local/hbase/bin/hbase org.apache.hadoop.hbase.io.hfile.HFile -a -b -e -k -p -f /hbase/data/default/person/498cbea2ddaf97f68c5d92ed9449cbf0/info/a8813e7736b7487ca21c5f0e841674b0
/usr/local/hbase/bin/hbase mapredcp
/usr/local/hadoop/bin/yarn jar /usr/local/hbase/lib/hbase-server-2.2.6.jar org.apache.hadoop.hbase.mapreduce.RowCounter student
/usr/local/hadoop/bin/yarn jar /usr/local/hbase/lib/hbase-server-2.2.6.jar org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns=HBASE_ROW_KEY,info:name,info:color fruit hdfs://node1:8020/input/hbasemr/fruit.csv
/usr/local/hadoop/bin/yarn jar /root/jar_test/hbase_mapred-1.0-SNAPSHOT.jar hbase_mapred_1.JobMain
/usr/local/hadoop/bin/yarn jar /root/jar_test/hbase_mapred-1.0-SNAPSHOT.jar hbase_mapred_2.JobMain2



create temporary function my_len as "hive_udf.MyUDF";
create temporary function my_explode as "hive_udtf.MyUDTF";



/usr/local/sqoop/bin/sqoop help
/usr/local/sqoop/bin/sqoop list-databases --connect jdbc:mysql://node3:3306/ --username root --password java521....
/usr/local/sqoop/bin/sqoop import --connect jdbc:mysql://node3:3306/company --username root --password java521.... --table staff --target-dir /sqoop/company --delete-target-dir --num-mappers 1 --fields-terminated-by "\t"
/usr/local/sqoop/bin/sqoop import --connect jdbc:mysql://node3:3306/company --username root --password java521.... --table staff --target-dir /sqoop/company --delete-target-dir --num-mappers 1 --fields-terminated-by "\t" --columns id,sex
/usr/local/sqoop/bin/sqoop import --connect jdbc:mysql://node3:3306/company --username root --password java521.... --table staff --target-dir /sqoop/company --delete-target-dir --num-mappers 1 --fields-terminated-by "\t" --where "id=1"
/usr/local/sqoop/bin/sqoop import --connect jdbc:mysql://node3:3306/company --username root --password java521.... --target-dir /sqoop/company --delete-target-dir --num-mappers 1 --fields-terminated-by "\t" --query 'select name,sex from staff where id <= 1 and $CONDITIONS;'
/usr/local/sqoop/bin/sqoop import --connect jdbc:mysql://node3:3306/company --username root --password java521.... --table staff --num-mappers 1 --fields-terminated-by "\t" --hive-import --hive-overwrite --hive-table staff_hive
/usr/local/sqoop/bin/sqoop import --connect jdbc:mysql://node3:3306/company --username root --password java521.... --table staff --num-mappers 1 --fields-terminated-by "\t" --columns "id,name,sex" --column-family "info" --hbase-create-table --hbase-row-key "id" --hbase-table "hbase_company" --split-by id
/usr/local/sqoop/bin/sqoop export --connect jdbc:mysql://node3:3306/company --username root --password java521.... --table staff --num-mappers 1 --input-fields-terminated-by "\t" --export-dir /user/hive/warehouse/staff_hive
/usr/local/sqoop/bin/sqoop --options-file /usr/local/sqoop/job/sqp.opt



/usr/local/solr/bin/solr start
/usr/local/solr/bin/solr create -c vertex_index -d /usr/local/solr/atlas_conf -shards 3 -replicationFactor 2
/usr/local/solr/bin/solr create -c edge_index -d /usr/local/solr/atlas_conf -shards 3 -replicationFactor 2
/usr/local/solr/bin/solr create -c fulltext_index -d /usr/local/solr/atlas_conf -shards 3 -replicationFactor 2
/usr/local/solr/bin/solr delete -c vertex_index