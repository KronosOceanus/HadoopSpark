/usr/local/zookeeper/bin/zkServer.sh start
/usr/local/hadoop/sbin/start-all.sh

/usr/local/zookeeper/bin/zkServer.sh stop
/usr/local/hadoop/sbin/stop-all.sh

/usr/local/zookeeper/bin/zkCli.sh -server node1:2181

hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.4-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 10MB
hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.4-tests.jar TestDFSIO -clean
hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.4.jar pi 2 10

hdfs oiv -i /usr/local/hadoop/data/namenodeData/current/fsimage_0000000000000153 -p XML -o my_fsimage.xml
hdfs oev -i edits_0000000000000000001-0000000000000000153 -p XML -o myedits.xml



hadoop.dll 下载
killed map！内存不够
Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. //
Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases
sparkalone-ha
mapred --daemon start historyserver



/usr/local/spark/sbin/start-all.sh
/usr/local/spark/sbin/start-history-server.sh
/usr/local/spark/bin/spark-shell --master spark://node1:7077
/usr/local/spark/sbin/stop-all.sh

val textFile = sc.textFile("hdfs://node1:8020/input/wordcount/wordcount.txt")
val counts = textFile.flatMap(line => line.split(","))
                 .map(word => (word, 1))
                 .reduceByKey(_ + _)
counts.saveAsTextFile("hdfs://node1:8020/output/wordcount_out2"

val NUM_SAMPLES = 100000
val count = sc.parallelize(1 to NUM_SAMPLES).filter { _ =>
  val x = math.random
  val y = math.random
  x*x + y*y < 1
}.count()
println(s"Pi is roughly ${4.0 * count / NUM_SAMPLES}")

/usr/local/spark/bin/spark-submit --master yarn --deploy-mode client --driver-memory 512m --driver-cores 1 --executor-memory 512m --num-executors 2 --executor-cores 1 --class org.apache.spark.examples.SparkPi /usr/local/spark/examples/jars/spark-examples_2.12-3.0.2.jar 10