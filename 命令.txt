/usr/local/zookeeper/bin/zkServer.sh start
/usr/local/hadoop/sbin/start-all.sh

/usr/local/zookeeper/bin/zkServer.sh stop
/usr/local/hadoop/sbin/stop-all.sh

/usr/local/zookeeper/bin/zkCli.sh -server node1:2181

hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.4-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 10MB
hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.4-tests.jar TestDFSIO -clean
hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.4.jar pi 2 10

hdfs oiv -i /usr/local/hadoop/data/namenodeData/current/fsimage_0000000000000153 -p XML -o my_fsimage.xml
hdfs oev -i edits_0000000000000000001-0000000000000000153 -p XML -o myedits.xml

 

hadoop.dll 下载
killed map！内存不够
Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. //
Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases
sparkalone-ha 搭建
mapred --daemon start historyserver
spark pom 依赖
idea 远程提交
kafka eagle
kafka 自定义提交 offset

尚硅谷大数据全套视频教程
下载链接：https://pan.baidu.com/s/18Feqa_63640xPB0fYJ8Ttg 
提取码：9bnr



/usr/local/spark/sbin/start-all.sh
/usr/local/spark/sbin/start-history-server.sh
/usr/local/spark/bin/spark-shell --master spark://node1:7077
/usr/local/spark/sbin/stop-all.sh
/usr/local/spark/sbin/stop-history-server.sh

val textFile = sc.textFile("hdfs://node1:8020/input/wordcount/wordcount.txt")
val counts = textFile.flatMap(line => line.split(","))
                 .map(word => (word, 1))
                 .reduceByKey(_ + _)
counts.saveAsTextFile("hdfs://node1:8020/output/wordcount_out2"

val NUM_SAMPLES = 100000
val count = sc.parallelize(1 to NUM_SAMPLES).filter { _ =>
  val x = math.random
  val y = math.random
  x*x + y*y < 1
}.count()
println(s"Pi is roughly ${4.0 * count / NUM_SAMPLES}")

/usr/local/spark/bin/spark-submit --master yarn --deploy-mode client --driver-memory 512m --driver-cores 1 --executor-memory 512m --num-executors 2 --executor-cores 1 --class org.apache.spark.examples.SparkPi /usr/local/spark/examples/jars/spark-examples_2.12-3.0.2.jar 10
/usr/local/spark/bin/spark-submit --master yarn --deploy-mode cluster --driver-memory 512m --driver-cores 1 --executor-memory 512m --num-executors 2 --executor-cores 1 --class spark_wordcount.WordCount /root/jar_test/spark_api-1.0-SNAPSHOT.jar hdfs://node1:8020/input/wordcount/wordcount.txt hdfs://node1:8020/output/wordcount_out3




/usr/local/flume/bin/flume-ng agent --conf /usr/local/flume/conf --conf-file /usr/local/flume/job/netcat-flume-logger.conf --name a1 -Dflume.root.logger=INFO,console
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/netcat-flume-logger.conf -n a1 -Dflume.root.logger=INFO,console
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/file-flume-logger.conf -n a1 -Dflume.root.logger=INFO,console
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/file-flume-hdfs.conf -n a1
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/dir-flume-hdfs.conf -n a1
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/files-flume-logger.conf -n a1 -Dflume.root.logger=INFO,console
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/mysource.conf -n a1 -Dflume.root.logger=INFO,console
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/mysink.conf -n a1 -Dflume.root.logger=INFO,console
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/netcat-flume-logger.conf -n a1 -Dflume.root.logger=INFO,console -Dflume.monitoring.type=ganglia -Dflume.monitoring.hosts=node1:8649
group1（a2,node1）（a3,node2）（a1,node3）
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/group1/flume1.conf -n a1
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/group1/flume2.conf -n a2
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/group1/flume3.conf -n a3
group2（a2,node1）（a3,node2）（a1,node3）
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/group2/flume1.conf -n a1
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/group2/flume2.conf -n a2 -Dflume.root.logger=INFO,console
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/group2/flume3.conf -n a3 -Dflume.root.logger=INFO,console
group3（a2,node1）（a3,node2）（a1,node3）
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/group3/flume1.conf -n a1
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/group3/flume2.conf -n a2 -Dflume.root.logger=INFO,console
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/group3/flume3.conf -n a3 -Dflume.root.logger=INFO,console
group4（a2,node1）（a3,node2）（a1,node3）
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/group4/flume1.conf -n a1
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/group4/flume2.conf -n a2
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/group4/flume3.conf -n a3 -Dflume.root.logger=INFO,console
group5（a2,node1）（a3,node2）（a1,node3）
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/group5/flume1.conf -n a1
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/group5/flume2.conf -n a2 -Dflume.root.logger=INFO,console
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/group5/flume3.conf -n a3 -Dflume.root.logger=INFO,console
flume + kafka
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/flume-kafka.conf -n a1
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /usr/local/flume/job/type-kafka.conf -n a1




/usr/local/kafka/bin/kafka-server-start.sh -daemon /usr/local/kafka/config/server.properties
/usr/local/kafka/bin/kafka-server-stop.sh /usr/local/kafka/config/server.properties
/usr/local/kafka/bin/kafka-topics.sh --list --zookeeper node1:2181
/usr/local/kafka/bin/kafka-topics.sh --create --zookeeper node1:2181 --topic first --partitions 2 --replication-factor 2
/usr/local/kafka/bin/kafka-topics.sh --delete --zookeeper node1:2181 --topic first
/usr/local/kafka/bin/kafka-topics.sh --describe --zookeeper node1:2181 --topic second
/usr/local/kafka/bin/kafka-console-producer.sh --topic first --broker-list node1:9092
/usr/local/kafka/bin/kafka-console-consumer.sh --topic first --bootstrap-server node1:9092 --consumer.config /usr/local/kafka/config/consumer.properties
/usr/local/kafka/bin/kafka-console-consumer.sh --topic first --bootstrap-server node1:9092 --from-beginning
/usr/local/kafka/bin/kafka-console-consumer.sh --topic __consumer_offsets --bootstrap-server node1:9092 --formatter "kafka.coordinator.group.GroupMetadataManager\$OffsetsMessageFormatter" --consumer.config /usr/local/kafka/config/consumer.properties --from-beginning



/usr/local/sqoop/bin/sqoop list-databases --connect jdbc:mysql://localhost:3306/ --username root --password java521....



/usr/local/hbase/bin/hbase-daemon.sh start master
/usr/local/hbase/bin/hbase-daemon.sh start regionserver
/usr/local/hbase/bin/hbase-daemon.sh stop master
/usr/local/hbase/bin/hbase-daemon.sh stop regionserver
/usr/local/hbase/bin/start-hbase.sh
/usr/local/hbase/bin/stop-hbase.sh
/usr/local/hbase/bin/hbase shell
/usr/local/hbase/bin/hbase org.apache.hadoop.hbase.io.hfile.HFile -a -b -e -k -p -f /hbase/data/default/person/498cbea2ddaf97f68c5d92ed9449cbf0/info/a8813e7736b7487ca21c5f0e841674b0
/usr/local/hbase/bin/hbase mapredcp
/usr/local/hadoop/bin/yarn jar /usr/local/hbase/lib/hbase-server-2.2.6.jar org.apache.hadoop.hbase.mapreduce.RowCounter student
/usr/local/hadoop/bin/yarn jar /usr/local/hbase/lib/hbase-server-2.2.6.jar org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns=HBASE_ROW_KEY,info:name,info:color fruit hdfs://node1:8020/input/hbasemr/fruit.csv
/usr/local/hadoop/bin/yarn jar /root/jar_test/hbase_mapred-1.0-SNAPSHOT.jar hbase_mapred_1.JobMain
/usr/local/hadoop/bin/yarn jar /root/jar_test/hbase_mapred-1.0-SNAPSHOT.jar hbase_mapred_2.JobMain2