# 结点负载均衡
start-balancer.sh -threshold 10
mapred --daemon start historyserver
mapred --daemon stop historyserver
# hadoop 测试
hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -Dmapreduce.output.fileoutputformat.compress=true -Dmapreduce.output.fileoutputformat.compress.codec=com.hadoop.compression.lzo.LzopCodec /input/word /ouput/word
hadoop jar /usr/local/hadoop/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /input/lzotest/bigtable.lzo
hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -Dmapreduce.job.inputformat.class=com.hadoop.mapreduce.LzoTextInputFormat /input/lzotest /ouput/lzotest
# hadoop 读写测试
hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -write -nrFiles 5 -fileSize 128MB
hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -read -nrFiles 5 -fileSize 128MB
hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar TestDFSIO -clean
# yarn 多队列测试
hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi -Dmapreduce.job.queuename=hive 2 10
hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi 2 10


# hive
set mapreduce.job.queuename=hive;
hiveserver2


# kafka 相关命令
/usr/local/kafka/bin/kafka-topics.sh --list --zookeeper node1:2181,node2:2181,node3:2181/kafka
/usr/local/kafka/bin/kafka-topics.sh --create --zookeeper node1:2181,node2:2181,node3:2181/kafka --topic first --partitions 3 --replication-factor 3
/usr/local/kafka/bin/kafka-topics.sh --delete --zookeeper node1:2181,node2:2181,node3:2181/kafka --topic first
/usr/local/kafka/bin/kafka-topics.sh --describe --zookeeper node1:2181,node2:2181,node3:2181/kafka --topic first
/usr/local/kafka/bin/kafka-console-producer.sh --topic first --broker-list node1:9092
/usr/local/kafka/bin/kafka-console-consumer.sh --topic first --bootstrap-server node1:9092 --from-beginning
/usr/local/kafka/bin/kafka-consumer-groups.sh --bootstrap-server node1:9092 --list
/usr/local/kafka/bin/kafka-consumer-groups.sh --bootstrap-server node1:9092 --describe --group console-consumer-98131
# kafka 压力测试
/usr/local/kafka/bin/kafka-producer-perf-test.sh --topic test --record-size 100 --num-records 100000 --throughput -1 --producer-props bootstrap.servers=node1:9092,node2:9092,node3:9092
/usr/local/kafka/bin/kafka-consumer-perf-test.sh --topic test --fetch-size 10000 --messages 100000 --threads 1 --broker-list node1:9092,node2:9092,node3:9092


# flume + kafka
/usr/local/kafka/bin/kafka-topics.sh --create --zookeeper node1:2181,node2:2181,node3:2181/kafka --topic topic_log --partitions 3 --replication-factor 3
/usr/local/kafka/bin/kafka-topics.sh --delete --zookeeper node1:2181,node2:2181,node3:2181/kafka --topic topic_log
/usr/local/kafka/bin/kafka-console-consumer.sh --topic topic_log --bootstrap-server node2:9092,node3:9092
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /opt/module/flume/job/taildir-kafka.conf -n a1 -Dflume.root.logger=INFO,console
/usr/local/flume/bin/flume-ng agent -c /usr/local/flume/conf -f /opt/module/flume/job/kafka-hdfs.conf -n a1 -Dflume.root.logger=INFO,console


# 数据生成
cd /opt/module/db_log && java -jar gmall2020-mock-db-2020-05-18.jar 


# sqoop，以 hadoop 为中心，向 hdfs 为导入
/usr/local/sqoop/bin/sqoop help
/usr/local/sqoop/bin/sqoop list-databases --connect jdbc:mysql://node3:3306/ --username root --password java521....